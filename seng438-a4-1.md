**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group \#:      |   1  |
| -------------- | --- |
| Student Names: |  Spencer van Roessel  |
|                |  Houssem Zaggar  |
|                |  Harris Hasnain  |
|                |  Kaylyn Tanton  |

# Introduction


# Mutation Testing


# Analysis of 10 Mutants of the Range class (killed or not)

1. getCentralValue method - Replaced double return with 0.0d for org/jfree/data/Range::getCentralValue → SURVIVED. This mutant replaces the return statement in the getCentralValue with a hard-coded 0.0 value. It survived because the test range had a lower bound of -3 and an upper bound of 3, making the central value 0. Therefore, the test did not fail with this mutant and it survived.

2. getCentralValue method - Replaced double addition with modulus → SURVIVED. This mutant replaced the addition in the statement lower / 2.0 + upper / 2.0 with modulus. Similarly to above, this mutant survived because of only having the single test with the range (-3, 3) and it was not able to detect the difference between the addition and modulus as they both equal 0.

3. getLength method - Negated double field upper → KILLED. All 3 test cases for the getLength method were able to detect the change when the upper field in the calculation was negated. Thus, the mutant was killed.

4. contains method - Replaced boolean return with false for org/jfree/data/Range::contains → KILLED. This mutant was easily killed because our test suite had test cases for values that were within the range as well as outside the range. The test case, test_contains_nearLowerBoundary, uses a value that is within a given range. This test would fail when false was returned by the mutant and thus, the mutant was killed.

5. contains method - Incremented (a++) double local variable number 1 → KILLED. There are two mutants that both appear to be identical. They both post-increment the local variable called "value" in the contains method. However, this mutant post-increments the first appearance of "value" in the statement: return (value >= this.lower && value <= this.upper); Although the post-increment does not affect the conditional value >= this.lower, it will increment "value" before performing the second conditional. In the test case test_contains_onUpperBoundary, 3 is tested in the range (-3, 3). When value = 3 is incremented to 4, the second part of the conditional fails and the test case is able to kill the mutant.

6. contains method - Incremented (a++) double local variable number 1 → SURVIVED. As stated above, this is the mutant that post-increments the second appearance of "value" in the conditional value <= this.upper. Since it is a post-increment, it first evaluates the conditional and then increments. After the contains method returns, the local variable "value" is irrelevant and therefore, does not impact the functionality of the method. As there is no reason or way to check the value of the local variable after the method runs, the mutant survives.

7. intersects method - Incremented (a++) double field lower → SURVIVED. This mutant is for the line: return (b1 >= this.lower); in the intersects method. Since it is a post-increment, the conditional is evaluated first and does not change or affect the return. However, it modified the lower field within a Range object. None of our test cases for the intersects method had assert statements to ensure that the lower and upper fields within the Range object did not change and that is why this mutant survived.

8. constrain method - Negated double local variable number 1 → KILLED. This mutant refers to the negation of the local variable and argument of the constrain method, "value", specifically in the line: double result = value; It is killed by multiple test cases. For example, test_constrain_within_range tests value = 1 in the range (-3, 3). Constrain returns the closest number to the input value that is within the range. Since 1 is within the range, it should return 1. However, with the mutant negating "value" to -1, -1 is returned instead and the mutant is killed.

9. constrain method - Removed conditional - replaced equality check with true → SURVIVED. This is an equivalent mutant that refers to the line of code: if (!contains(value)). When this conditional is replaced with true, the functionality of the program does not change. The argument "value" is stored in a local variable called "result" prior to this conditional. For tests where the value is not contained within the range, it will behave as expected, checking whether it is above or below the range. However, if the value is within the test range, neither of the if statements within this conditional will be executed because they check for if the value is less than the lower bound or greater than the upper bound. Since it is within the range, the "result" variable will not be modified and it would be as if the outer if block was not executed, even though it evaluated to true in the mutation.

10. combine method - Replaced return value with null for org/jfree/data/Range::combine → SURVIVED. This mutant refers to lines of code:  if (range1 == null) {return range2;}. We had two test cases that dealt with null ranges. The first one, test_combine_bothRangeNull, used two null Range objects. This test would not catch the mutant because the proper execution would be to return range2 which is null. The second test, test_combine_oneRangeNull, has range1 be valid, and range2 be null. With this test, the if conditional is evaluated to false and therefore the return statement is not run. To kill the mutant, a test case would need to be added where range1 is null, so the if statement executes, and where range2 is valid. When the mutant returns null, the new test would fail and the mutant would be killed.


# Report all the statistics and the mutation score for each of the mutated classes

Range class:

Before:

<img width="647" alt="test suite before" src="https://github.com/seng438-winter-2024/seng438-a4-SpencerVR1/assets/113068550/5419231d-7602-40ab-b238-9cefa0ae8d28">

After:

<img width="647" alt="test suite after" src="https://github.com/seng438-winter-2024/seng438-a4-SpencerVR1/assets/113068550/a04e15ff-fbf1-4789-babf-4086d20054a2">


DataUtilities class:

Before:

<img alt="test suite before" src="https://github.com/seng438-winter-2024/seng438-a4-SpencerVR1/blob/main/DataUtilities_Results/old_suite_coverage.jpg">

After:

<img alt="test suite after" src="https://github.com/seng438-winter-2024/seng438-a4-SpencerVR1/blob/main/DataUtilities_Results/new_suite_coverage.jpg">

Note for DataUtilities: The initial mutation coverage for the test suite was already extremely high at 91%, with most of the surviving mutants being equivalent mutants (post incremennt after returning a value, less than to not equals in a for loop, etc). Given the fact that equivalent mutants cannot be killed, after analyzing the mutation log and creating tests to kill the remaining non-equivalent mutants, mutation coverage could only be increased to a maximum value of 93% for the class, unable to go higher due to only equivalent mutants remaining.


# A discussion on the effect of equivalent mutants on mutation score accuracy / how they are detected (approach for finding equivalent mutants, benefits, disadvantages, assumptions)


# A discussion of what was done to improve the mutation score of the test suites (design strategy)


# Why do we need mutation testing? Advantages and disadvantages of mutation testing


# Web Testing


# Explain your SELENUIM test case design process
As a group, we selected a total of eight GUI website functions to test. These included login, colour and size selection, cart, checkout, favourite, menu selection, quantity selection, and search. Different baseline inputs and event combinations were tested for each functionality. The test suite was designed as a group through a preliminary exploration of the website under test. By working together, the standard and depth of each function's tests was kept consistent. Test cases varied for different functionalities. For example, testing the favourite button involved testing different numbers of items being favourited, or removing favourites, while testing the checkout page involved following different paths to get to the same checkout screen. Then the design, automation, and execution of the tests were completed through pair programming. Some test cases were altered and added throughout the automation with Selenium. 
# Explain the use of assertions and checkpoints
Assertions and verifications were added through one of two methods. Either they were implemented during the recording automation of a test by right-clicking a target, or they were added after the recording by manually inserting a command and setting the target and value to verify a component of the website. Each verification was designed purposefully to ensure that the test case being implemented was performing as expected. For instance, in the favourite one item test, a "verify element present" check was added to make sure that the selected item was in fact in the favourites page. Typically, multiple verifications were inputted as checkpoints throughout a single test case to make it more organized and obvious where an error may have occurred if a test were to fail. A challenge that arose from assertions was the slight delay in website reaction, which caused assertions to fail if the selenium tests ran too quickly. Specifically, in the test case Test_Login:Valid_Username_Password the verification added to ensure that the "Sign in" button now displayed "Hi <username>" was failing because the SUT was delayed. To solve this issue, the verification was changed to first hover over the previous button, then check that the "log out" text appeared in the dropdown menu. There was one other case in a quantity function test where the test execution speed had to be set slightly below the fastest setting for the verification to correctly check an element when it was changed in the previous  command. 
# how did you test each functionaity with different test data
As described above, different functionalities required different variations in test data. Some tests varied data by changing the inputs such as the tests for login, quantities, and search. Unique numbers in quantities were tested, and valid, half complete, fully complete, and invalid searches were tested. The cart and menu bar functions tested different paths in their pages. For example, the menu bar was used to navigate the site by the primary categories, secondary drop dropdowns, and even more specific selections that appeared when hovering over a secondary drop down item. Some difficulties in developing detailed and repeatable tests arose when testing the colour and size selections of a specific item. If a colour selection test was developed for the first item in a page with a verification that the colour displayed matched the one chosen, the tests that pass one day may fail another day if the first item in the page changed. In order to write effective tests, the items selected had to be pre chosen for each test case, so that a test that selected all the seven different colour options in one item wouldn't run on another item with only three colour options. 
# Discuss advantages and disadvantages of Selenium vs. Sikulix
Through this lab experience, we found Selenium offered a significant advantage due to its automated testing capabilities. With Selenium, the testing process was streamlined by automating clicks, scrolls, inputs, etc. to capture the performed actions and convert them into repeatable code very quickly. SikuliX requires each separate command to be manually entered (with the option to snapshot an element to obtain its ID), which resulted in a longer test design process that demanded more specialized knowledge to properly implement. However, there were some occasional limitations with Selenium. There were several instances where it failed to capture certain actions accurately. For example, when hovering over a button to reveal a dropdown menu, Selenium sometimes missed the hover command, leading to difficulties in locating the button. The test script would time out, and be unable to complete the rest of the test case. To address this issue, we had to pinpoint where the automation stopped functioning correctly and manually add a "mouse over" command to correct the test script. SikuliX did not require debugging to solve issues like this because each command was sequentially inputted by the tester. SikuliX also provided additional flexibility in testing. One notable feature is the ability to precisely control where an icon is clicked, developing more detailed and specialized testing scenarios.

# How the team work/effort was divided and managed
To complete Part 1, we split into pairs based on Java classes like previous labs. One pair ran mutation testing and designed new test cases to improve the mutation score for Range while the other pair did DataUtilities. Then, we all worked together to design test cases for Part 2, first by creating a list of functionalities to test, and then splitting them. This ensured an equal workload distribution and that all group members got to go through the lab's full analysis, design, and implementation procedure.  

# Difficulties encountered, challenges overcome, and lessons learned
The biggest challenge was learning how to use PITest. Some of us had trouble getting the summary report to show up after running it for the first time. Another challenge with PITest was how long the tests would take to run on most of our computers. This made it harder to make changes since you couldn't see the effect immediately. 

There were also some challenges while trying to learn how to use Selenium. While automated testing was useful, it didn't work perfectly and there were instances where Selenium missed a command and we had to manually change the test script. 

# Comments/feedback on the lab itself

